# 学習に関するテクニック

* SGDとは、確率的勾配降下法のこと
  * 勾配方向に向かう一番単純な方法
  * stochastic gradient descent
* SGDは解く関数の形状が等方的でないと収束がうまくいかない
  * 勾配の方向が本来の最小値の方向を指しているとは限らない
    * この辺が、レーベンバーグだと解決されているのか？
      * 自分の理解が正しければ、方向というよりは、収束点がわかっているイメージ
* Momentum
  * 収束に慣性を持たせる
* AdaGrad
  * パラメータ毎に適応的に学習係数を調整する
  * 各パラメータの勾配の二乗和（スカラー）を逐次的に足し合わせてその分だけ学習係数を小さくする
  * 結構イマイチな気がする
  * RMSPropという方法の方が良さそう
* Adam
  * MomentumとAdaGradの融合
  * ライブラリを使おう
* 初期値はとりあえずランダムで良い?
  * 何か学習済みのパラメータがあればそれを流用するのが良い
* 勾配消失問題
  * 
* 「表現力の制限」の問題は、逆にニューラルネットをコンパクトにしたい際に使用されている気がする
* 初期値について
  * Xavierの初期値という考え方がある
    * できるだけ偏りなく勾配消失しない形でアクティベーションされることを目指したのかな
    * ところで、活性化関数に用いる関数は、減点対象であることが望ましい性質として知られているらしい
  * 活性化関数にReLUを使う場合は、「Heの初期値」を用いると良い
    * 考え方はほとんどXavierと変わらない
    * 原点の負の領域を使わないので
    ```math
      \sqrt(\frac{1}{n})
    ```
    のかわりに
    ```math
      \sqrt(\frac{2}{n})
    ```
    を使うだけ
* Batch Nomalization
  * 恥ずかしいことに、バッチノーマライゼーションとミニバッチ学習を混同していたが、全然違う
  * ミニバッチ学習
	* （ちょっとまだ自信がないが）ミニバッチ学習をせずに一つのデータだけで勾配を計算してしまうと、勾配方法が信用できなくなってしまうという話だと思う
	* 100個とか適当にサンプリングしたデータの塊を全データの近似とみなすということは、勾配方向がそこそこだ正しいとみなすことかな？
	* なので、ミニバッチ学習のサンプリング数は小さぎてはいけないと思う
  * Batch Normalizationは、ニューラルネットにBatch Nomaizationレイヤを挿入する
  * 角層でのアクティベーションの分布を適度な広がりになるように調整する専用の層を設計するということ
	* ミニバッチごとに正規化を行う
	* データの分布が平均が０で分散が１になるように正規化
	* 逆伝播の解説は省略されていたが、定性的な理解をしたうえで、基本的にライブラリを信じれば良いだろう
* 過学習を抑制する手段として下の手法が有名
  * Weidht decay
	* 重みパラメータにペナルティをつける
	* 割と昔からある考え方
  * Dropout
	* 訓練時に、わざとランダムに使用しないニューロンを設定する
	* 擬似的に、アンサンブル学習を行っているという解釈になる
* 最近はBatch Normalizationの方が有力ということなんだろうか
* ハイパーパラメータの最適化
  * 今後重要な領域だろうが、今は興味外なので省略
  * ハイパーパラメータを遺伝的アルゴリズムで最適化とかあった気がする
