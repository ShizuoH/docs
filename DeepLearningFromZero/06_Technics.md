# 学習に関するテクニック

* SGDとは、確率的勾配降下法のこと
  * 勾配方向に向かう一番単純な方法
  * stochastic gradient descent
* SGDは解く関数の形状が等方的でないと収束がうまくいかない
  * 勾配の方向が本来の最小値の方向を指しているとは限らない
    * この辺が、レーベンバーグだと解決されているのか？
      * 自分の理解が正しければ、方向というよりは、収束点がわかっているイメージ
* Momentum
  * 収束に慣性を持たせる
* AdaGrad
  * パラメータ毎に適応的に学習係数を調整する
  * 各パラメータの勾配の二乗和（スカラー）を逐次的に足し合わせてその分だけ学習係数を小さくする
  * 結構イマイチな気がする
  * RMSPropという方法の方が良さそう
* Adam
  * MomentumとAdaGradの融合
  * ライブラリを使おう
* 初期値はとりあえずランダムで良い?
  * 何か学習済みのパラメータがあればそれを流用するのが良い
* 勾配消失問題
  * 
* 「表現力の制限」の問題は、逆にニューラルネットをコンパクトにしたい際に使用されている気がする
* 初期値について
  * Xavierの初期値という考え方がある
    * できるだけ偏りなく勾配消失しない形でアクティベーションされることを目指したのかな
    * ところで、活性化関数に用いる関数は、減点対象であることが望ましい性質として知られているらしい
  * 活性化関数にReLUを使う場合は、「Heの初期値」を用いると良い
    * 考え方はほとんどXavierと変わらない
    * 原点の負の領域を使わないので
    ```math
      \sqrt(\frac{1}{n})
    ```
    のかわりに
    ```math
      \sqrt(\frac{2}{n})
    ```
    を使うだけ
